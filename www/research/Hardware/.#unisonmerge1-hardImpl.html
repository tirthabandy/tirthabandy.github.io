
<html>
<head>
<title>Hardware Implementation</title>
</head>
<body>

<h1>Hardware Implementation</h1>
<hr>

<!---------------------------------------------------------------->
<h2>Comparison with Visual Servo</h2>
<par>
This experiment  compares the performance of the visual servo tracker to our vantage tracker. 
The robot identifies the legs of the target (here the girl in the scene) and moves towards it.
In the experiment the target is kept stationary in the beginning and a box is pushed with the 
objective of occluding the robot's visibility of the target.  
We see that while visual servo loses the target, the vantage tracker is able to adapt to the changing
environment and successfully follow the target.
 ( The obstacle avoidance behavior is 
turned off so as not to interfere in the robot's interaction with the environment.)


<table cellpadding="10" cellspacing="15" border="1" frame="hsides" bgcolor="#A1A1A1" width="90%" align="center">
<tr>
	<td><a href="servo-lost.ogg"><img src="servo-lost.png" width="200" height="" border="1" alt="Visual Servo tracker" align=""></a></td>
	<td>The visual servo algorithm does not take into account the changing environment and allows the target to be occluded 
	by the box.</td>	
	
</tr>
<tr>
	<td><a href="vantage.sit.ogg"><img src="vantage.sit.png" width="200" height="" border="1" alt="Vantage tracker" align=""></a></td>
	<td>The vantage tracker explicitly models the visibility relationship of the target, itself and the occlusion due to the box at each instant.
	It identifies the risk of losing the target to the occlusion generated due to the box and reduces this risk by swinging out 
	inorder to keep the target in view. The inset at the bottom-right in the video depicts the visibility and the motion decision of the robot. The blue lines 
	denote the occlusion edge, and the green line is the next motion decision. <b>Note</b>: that at the beginning of the video the robot turns to its right to adjust and plan for the presence of occlusions produced by the box, instead of just moving straight towards the target. </td>
</tr>
</table>

<!---------------------------------------------------------------->
<h2>Guarding against limited Field of View</h2>
The visibility sensor is the SICKLms 200 which has an FoV of 180deg. Due to this the robot must not only handle occlusions generated
	by the environmental agents but also guard againts it own sensing limits. The FoV limits are modeled similar to the occlusion edges and the 
	robot rotates to keep its FoV limits away from the target.
<table cellpadding="10" cellspacing="15" border="1" frame="hsides" bgcolor="#A1A1A1" width="90%" align="center">
<tr>
	<td><a href="turning.ogg"><img src="turning.png" width="200" height="" border="1" alt="Limited FoV." align=""></a></td>
	<td> When the target turns and doubles back on the robot, it challenges the FoV limits and the robot turns to keep the target away from the 
	FoV limits.  
	</td>	
	</tr>
	</table>

<!---------------------------------------------------------------->
<h2>Turning a Corner</h2>
One of the most common occurences that needs to be handled by the robot in the target following scenario is when the target turns
around the corner. In such a case the robot must plan its motion such that it can negotiate the turn while keeping the target in view.
This requires balancing the immediate goal of swinging out to keep the target to the future goal of eliminating this occlusion edge.    
<table cellpadding="10" cellspacing="15" border="1" frame="hsides" bgcolor="#A1A1A1" width="90%" align="center"> 
<tr>
	<td><a href="corner-moving.1.ogg"><img src="corner-moving.1.png" width="200" height="" border="1" alt="Turning a Corner." align=""></a></td>
	<td> We see in the video that in the beginning the robot gives more priority to moving towards the corner but when the target is close to the 
	 occlusion edge and is in danger of being lost the robot has to swing out to keep the target in view. It is able to make a successful turn because 
	 by moving towards the corner in the beginning, it has acheived a relative vantage position from where it can swing out more effectively.
	</td>	
	</tr>
	</table>

<!---------------------------------------------------------------->
<h2>Target following in a cluttered environment</h2>
This video shows the robot following a target through a cluttered environment.
<table cellpadding="10" cellspacing="15" border="1" frame="hsides" bgcolor="#A1A1A1" width="90%" align="center"> 
<tr>
	<td><a href="cluttered.ogg"><img src="cluttered.png" width="200" height="" border="1" alt="Highly cluttered env." align=""></a></td>
	<td>The chairs in the lobby generate a large number of 
	escape edges that the robot has to handle. Note that the robot tends to generate motion decisions biased towards left as there are larger
	number of escape edges there.
	Due to the online nature and fast computation of the risk the robot is able to successfully follow the target around.	
	</td>
</tr>

<tr>
<td><b>latest results!!</b> <br> The videos will be uploaded soon ...</td> 
<td> The robot is able to detect and follow the person even in the presence of other people and momentary occlusions. The robot was able to successfully follow the target in and around the school cafeteria and back. 
</td>
<!-- 	
	<td><a href="cluttered.ogg"><img src="" width="" height="" border="0" alt="Highly cluttered env." align=""></a></td>
	<td>The chairs in the lobby generate a large number of 
	escape edges that the robot has to handle. Note that the robot tends to generate motion decisions biased towards left as there are larger
	number of escape edges there.
	Due to the online nature and fast computation of the risk the robot is able to successfully follow the target around.	
	</td> 
	-->	
</tr>	

</table>



</body>
</html>